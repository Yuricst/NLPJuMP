var documenterSearchIndex = {"docs":
[{"location":"api_core.html#Core-routines","page":"Core","title":"Core routines","text":"","category":"section"},{"location":"api_core.html#Construct-model","page":"Core","title":"Construct model","text":"","category":"section"},{"location":"api_core.html","page":"Core","title":"Core","text":"Modules = [NLPSaUT]\nOrder   = [:function, :type]\nPages   = [\n  \"model.jl\",\n]","category":"page"},{"location":"api_core.html#Main.NLPSaUT.build_model!-Tuple{JuMP.Model, Function, Int64, Int64, Int64, Vector, Vector, Vector}","page":"Core","title":"Main.NLPSaUT.build_model!","text":"build_model!(model::JuMP.Model, f_fitness::Function, nx::Int, nh::Int, ng::Int, lx::Vector, ux::Vector, x0::Vector=nothing)\n\nExtend model for NLP problem via memoized fitness function.\n\nArguments:\n\nmodel::JuMP.Model: model to append objective and constraints\nf_fitness::Function: fitness function, returning [f, h, g]\nnx::Int: number of decision variables \nnh::Int: number of equality constraints \nng::Int: number of inequality constraints \nlx::Vector: lower bounds on decision variables \nux::Vector: upper bounds on decision variables \nx0::Vector: initial guess\nauto_diff::Bool: whether to use automatic differentiation\norder::Int: order of FiniteDifferences, minimum is 2\nfd_type::String: finite-difference method, \"forward\", \"backward\", or \"central\"\n\n\n\n\n\n","category":"method"},{"location":"api_core.html#Main.NLPSaUT.build_model-Tuple{Any, Function, Int64, Int64, Int64, Vector, Vector, Vector}","page":"Core","title":"Main.NLPSaUT.build_model","text":"build_model(f_fitness::Function, nx::Int, nh::Int, ng::Int, lx::Vector, ux::Vector, x0::Vector=nothing, fd_type::Function=nothing, order::Int=2)\n\nBuild model for NLP problem with memoized fitness function.\n\nArguments:\n\noptimizer: optimizer to use with the model\nf_fitness::Function: fitness function, returning [f, h, g]\nnx::Int: number of decision variables \nnh::Int: number of equality constraints \nng::Int: number of inequality constraints \nlx::Vector: lower bounds on decision variables \nux::Vector: upper bounds on decision variables \nx0::Vector: initial guess\nauto_diff::Bool: whether to use automatic differentiation\norder::Int: order of FiniteDifferences, minimum is 2\nfd_type::String: finite-difference method, \"forward\", \"backward\", or \"central\"\n\n\n\n\n\n","category":"method"},{"location":"api_core.html#Memoization","page":"Core","title":"Memoization","text":"","category":"section"},{"location":"api_core.html","page":"Core","title":"Core","text":"Modules = [NLPSaUT]\nOrder   = [:function, :type]\nPages   = [\n  \"memoize.jl\",\n]","category":"page"},{"location":"api_core.html#Main.NLPSaUT.memoize_fitness-Tuple{Function, Int64}","page":"Core","title":"Main.NLPSaUT.memoize_fitness","text":"memoize_fitness(f_fitness::Function, n_outputs::Int)\n\nMemoize fitness function.  Because foo_i is auto-differentiated with ForwardDiff, our cache needs to work when x is a Float64 and a ForwardDiff.Dual.\n\nSee:  https://jump.dev/JuMP.jl/stable/tutorials/nonlinear/tipsandtricks/#Memoization\n\n\n\n\n\n","category":"method"},{"location":"api_core.html#Main.NLPSaUT.memoize_fitness_gradient","page":"Core","title":"Main.NLPSaUT.memoize_fitness_gradient","text":"memoize_fitness_gradient(f_fitness::Function, nfitness::Int, fd_type::Function, order::Int=2)\n\nCreate memoized gradient computation with method specified by fd_type     - fd_type = \"forward\" use FiniteDifferences.forward_fdm()     - fd_type = \"central\" use FiniteDifferences.central_fdm()     - fd_type = \"backward\" use FiniteDifferences.backward_fdm()\n\n\n\n\n\n","category":"function"},{"location":"index.html#NLPSaUT","page":"Home","title":"NLPSaUT","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"The NLPSaUT module constructs a JuMP model for a generic nonlinear program (NLP). The expected use case is solving a differentiable (either analytically or numerically) nonconvex NLP with gradient-based algorithms such as Ipopt or SNOPT. ","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"The user is expected to provide a \"fitness function\" (pygmo-style), which evaluates the objective, equality, and inequality constraints. Below is an example: ","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"function f_fitness(x::T...) where {T<:Real}\n    # compute objective\n    f = x[1]^2 - x[2]\n    \n    # equality constraints\n    h = zeros(T, 1)\n    h = x[1]^3 + x[2] - 2.4\n\n    # inequality constraints\n    g = zeros(T, 2)\n    g[2] = -0.3x[1] + x[2] - 2   # y <= 0.3x + 2\n    g[1] = x[1] + x[2] - 5      # y <= -x + 5\n    return [f; h; g]\nend","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Derivatives of f_fitness is taken using ForwardDiff.jl (which is the default JuMP behavior according to its docs); as such, f_fitness should be written in a way that is compatiable to ForwardDiff.jl (read here as to why it is ForwardDiff, not ReverseDiff).  For reference, here's the JuMP docs page on common mistakes when using ForwardDiff.jl. ","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"The model constructed by NLPSaUT utilizes memoization to economize on the fitness evaluation (see JuMP Tips and tricks on NLP). ","category":"page"},{"location":"index.html#Quick-start","page":"Home","title":"Quick start","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"git clone this repository\nstart julia-repl\nactivate & instantiate package (first time)","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"pkg> activate .\njulia> using Pkg                # first time only\njulia> Pkg.instantiate()        # first time only","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"run tests","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"(NLPSaUT) pkg> test","category":"page"},{"location":"index.html#Note","page":"Home","title":"Note","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"To use with SNOPT, it's probably better to go through GAMS.jl rather than to use SNOPT7.jl directly (installing SNOPT7.jl currently errors on julia v1.10)","category":"page"}]
}
