{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04990884",
   "metadata": {},
   "source": [
    "# Implementation spatted out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c4b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64cabd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Main.NLPSaUT"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"../src/NLPSaUT.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fcf98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memoize_fitness (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function memoize_fitness(f_fitness::Function, nx::Int, n_outputs::Int)\n",
    "    last_x, last_f = ones(nx), ones(n_outputs)\n",
    "    last_dx, last_dfdx = ones(nx), ones(nx)\n",
    "    function f_fitness_i(i::Int, x::T...) where {T<:Real}\n",
    "        if T == Float64\n",
    "            if x != last_x\n",
    "                last_x, last_f = x, f_fitness(x...)\n",
    "            end\n",
    "            return last_f[i]::T\n",
    "        else\n",
    "            if x != last_dx\n",
    "                last_dx, last_dfdx = x, f_fitness(x...)\n",
    "            end\n",
    "            return last_dfdx[i]::T\n",
    "        end\n",
    "    end\n",
    "    return [(x...) -> f_fitness_i(i, x...) for i in 1:n_outputs]\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f51df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_fitness (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f_fitness(x...)    \n",
    "    # objective\n",
    "    f = x[1]^2 - x[2]\n",
    "\n",
    "    # equality constraints\n",
    "    h = zeros(1,)\n",
    "    h = x[1]^3 + x[2] - 1\n",
    "\n",
    "    # inequality constraints\n",
    "    g = zeros(2,)\n",
    "    g[1] = x[1]^2 + x[2]^2 - 1\n",
    "    g[2] = x[2] - 2\n",
    "\n",
    "    fitness = vcat(f,h,g)[:]\n",
    "    return fitness\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24abb807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 1.2\n",
       " 0.9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# problem dimensions\n",
    "nx = 2                   # number of decision vectors\n",
    "nh = 1                   # number of equality constraints\n",
    "ng = 2                   # number of inequality constraints\n",
    "nfitness = 1 + nh + ng\n",
    "lx = -10*ones(nx,)\n",
    "ux =  10*ones(nx,)\n",
    "x0 = [1.2, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6979758e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{var\"#2#5\"{Int64, var\"#f_fitness_i#3\"{typeof(f_fitness)}}}:\n",
       " #2 (generic function with 1 method)\n",
       " #2 (generic function with 1 method)\n",
       " #2 (generic function with 1 method)\n",
       " #2 (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoized_fitness  = memoize_fitness(f_fitness, nx, nfitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17af7b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Main.NLPSaUT.var\"#7#15\"{Int64, Main.NLPSaUT.var\"#foo_fwd#13\"{Int64, Main.NLPSaUT.var\"#f_fitness_nosplat#12\"{typeof(f_fitness)}}}}:\n",
       " #7 (generic function with 1 method)\n",
       " #7 (generic function with 1 method)\n",
       " #7 (generic function with 1 method)\n",
       " #7 (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇memoized_fitness = NLPSaUT.memoize_fitness_gradient(\n",
    "    f_fitness, nx, nfitness, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd6e657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memoized_fitness[1]([0.1,0.2]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ff22f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Unable to register the function :f.\n\nCommon reasons for this include:\n\n * The function takes `f(x::Vector)` as input, instead of the splatted\n   `f(x...)`.\n * The function assumes `Float64` will be passed as input, it must work for any\n   generic `Real` type.\n * The function allocates temporary storage using `zeros(3)` or similar. This\n   defaults to `Float64`, so use `zeros(T, 3)` instead.\n\nAs examples, instead of:\n```julia\nmy_function(x::Vector) = sum(x.^2)\n```\nuse:\n```julia\nmy_function(x::T...) where {T<:Real} = sum(x[i]^2 for i in 1:length(x))\n```\n\nInstead of:\n```julia\nfunction my_function(x::Float64...)\n    y = zeros(length(x))\n    for i in 1:length(x)\n        y[i] = x[i]^2\n    end\n    return sum(y)\nend\n```\nuse:\n```julia\nfunction my_function(x::T...) where {T<:Real}\n    y = zeros(T, length(x))\n    for i in 1:length(x)\n        y[i] = x[i]^2\n    end\n    return sum(y)\nend\n```\n\nReview the stacktrace below for more information, but it can often be hard to\nunderstand why and where your function is failing. You can also debug this\noutside of JuMP as follows:\n```julia\nimport ForwardDiff\n\n# If the input dimension is 1\nx = 1.0\nmy_function(a) = a^2\nForwardDiff.derivative(my_function, x)\n\n# If the input dimension is more than 1\nx = [1.0, 2.0]\nmy_function(a, b) = a^2 + b^2\nForwardDiff.gradient(x -> my_function(x...), x)\n```\n",
     "output_type": "error",
     "traceback": [
      "Unable to register the function :f.\n\nCommon reasons for this include:\n\n * The function takes `f(x::Vector)` as input, instead of the splatted\n   `f(x...)`.\n * The function assumes `Float64` will be passed as input, it must work for any\n   generic `Real` type.\n * The function allocates temporary storage using `zeros(3)` or similar. This\n   defaults to `Float64`, so use `zeros(T, 3)` instead.\n\nAs examples, instead of:\n```julia\nmy_function(x::Vector) = sum(x.^2)\n```\nuse:\n```julia\nmy_function(x::T...) where {T<:Real} = sum(x[i]^2 for i in 1:length(x))\n```\n\nInstead of:\n```julia\nfunction my_function(x::Float64...)\n    y = zeros(length(x))\n    for i in 1:length(x)\n        y[i] = x[i]^2\n    end\n    return sum(y)\nend\n```\nuse:\n```julia\nfunction my_function(x::T...) where {T<:Real}\n    y = zeros(T, length(x))\n    for i in 1:length(x)\n        y[i] = x[i]^2\n    end\n    return sum(y)\nend\n```\n\nReview the stacktrace below for more information, but it can often be hard to\nunderstand why and where your function is failing. You can also debug this\noutside of JuMP as follows:\n```julia\nimport ForwardDiff\n\n# If the input dimension is 1\nx = 1.0\nmy_function(a) = a^2\nForwardDiff.derivative(my_function, x)\n\n# If the input dimension is more than 1\nx = [1.0, 2.0]\nmy_function(a, b) = a^2 + b^2\nForwardDiff.gradient(x -> my_function(x...), x)\n```\n",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:35",
      " [2] _validate_register_assumptions(f::var\"#2#5\"{Int64, var\"#f_fitness_i#3\"{typeof(f_fitness)}}, name::Symbol, dimension::Int64)",
      "   @ MathOptInterface.Nonlinear ~/.julia/packages/MathOptInterface/fTxO0/src/Nonlinear/operators.jl:267",
      " [3] (MathOptInterface.Nonlinear._MultivariateOperator{2})(op::Symbol, f::Function)",
      "   @ MathOptInterface.Nonlinear ~/.julia/packages/MathOptInterface/fTxO0/src/Nonlinear/operators.jl:297",
      " [4] register_operator(registry::MathOptInterface.Nonlinear.OperatorRegistry, op::Symbol, nargs::Int64, f::Function)",
      "   @ MathOptInterface.Nonlinear ~/.julia/packages/MathOptInterface/fTxO0/src/Nonlinear/operators.jl:350",
      " [5] register_operator",
      "   @ ~/.julia/packages/MathOptInterface/fTxO0/src/Nonlinear/model.jl:217 [inlined]",
      " [6] register(model::Model, op::Symbol, dimension::Int64, f::Function; autodiff::Bool)",
      "   @ JuMP ~/.julia/packages/JuMP/9CBpS/src/nlp.jl:677",
      " [7] top-level scope",
      "   @ In[9]:8"
     ]
    }
   ],
   "source": [
    "# set variables\n",
    "model = Model(Ipopt.Optimizer)\n",
    "@variable(model, lx[i] <= x[i=keys(lx)] <= ux[i], start = x0[i])\n",
    "hs = [Symbol(\"h\"*string(idx)) for idx = 1:nh]\n",
    "gs = [Symbol(\"g\"*string(idx)) for idx = 1:ng]\n",
    "        \n",
    "#register(model, :f,  nx, memoized_fitness[1], ∇memoized_fitness[1])\n",
    "register(model, :f,  nx, memoized_fitness[1]; autodiff=true)\n",
    "\n",
    "# # append equality constraints\n",
    "# for ih = 1:nh\n",
    "#     #register(model, hs[ih], nx, memoized_fitness[1+ih], ∇memoized_fitness[1+ih])\n",
    "#     register(model, hs[ih], nx, memoized_fitness[1+ih]; autodiff=true)\n",
    "# end\n",
    "\n",
    "# # append inequality constraints\n",
    "# for ig = 1:ng\n",
    "#     #register(model, gs[ig], nx, memoized_fitness[1+nh+ig], ∇memoized_fitness[1+nh+ig])\n",
    "#    register(model, gs[ig], nx, memoized_fitness[1+nh+ig]; autodiff=true)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19212c9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Unrecognized function \"f\" used in nonlinear expression.\n\nYou must register it as a user-defined function before building\nthe model. For example, replacing `N` with the appropriate number\nof arguments, do:\n```julia\nmodel = Model()\nregister(model, :f, N, f, autodiff=true)\n# ... variables and constraints ...\n```\n",
     "output_type": "error",
     "traceback": [
      "Unrecognized function \"f\" used in nonlinear expression.\n\nYou must register it as a user-defined function before building\nthe model. For example, replacing `N` with the appropriate number\nof arguments, do:\n```julia\nmodel = Model()\nregister(model, :f, N, f, autodiff=true)\n# ... variables and constraints ...\n```\n",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:35",
      " [2] assert_registered(registry::MathOptInterface.Nonlinear.OperatorRegistry, op::Symbol, nargs::Int64)",
      "   @ MathOptInterface.Nonlinear ~/.julia/packages/MathOptInterface/fTxO0/src/Nonlinear/operators.jl:427",
      " [3] macro expansion",
      "   @ ~/.julia/packages/JuMP/9CBpS/src/macros.jl:1925 [inlined]",
      " [4] top-level scope",
      "   @ In[10]:2"
     ]
    }
   ],
   "source": [
    "args = Array(x)\n",
    "@NLobjective(model, Min, f(args...))\n",
    "#@NLobjective(model, Min, f(x...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424a11d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @NLconstraint(model, h1(x...) == 0)\n",
    "# @NLconstraint(model, -Inf <= g1(x...) <= 0)\n",
    "# @NLconstraint(model, -Inf <= g2(x...) <= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31eec8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.14.4, running with linear solver MUMPS 5.5.1.\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        0\n",
      "                variables with lower and upper bounds:        2\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  0.0000000e+00 0.00e+00 0.00e+00  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  0.0000000e+00 0.00e+00 0.00e+00  -1.0 1.20e-02    -  1.00e+00 1.00e+00f  1\n",
      "   2  0.0000000e+00 0.00e+00 0.00e+00  -2.5 3.36e-02    -  1.00e+00 1.00e+00f  1\n",
      "   3  0.0000000e+00 0.00e+00 0.00e+00  -3.8 6.14e-02    -  1.00e+00 1.00e+00f  1\n",
      "   4  0.0000000e+00 0.00e+00 0.00e+00  -5.7 1.34e-02    -  1.00e+00 1.00e+00f  1\n",
      "   5  0.0000000e+00 0.00e+00 0.00e+00  -8.6 1.47e-03    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 5\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Dual infeasibility......:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Variable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   2.7760730773788619e-09    2.7760730773788619e-09\n",
      "Overall NLP error.......:   2.7760730773788619e-09    2.7760730773788619e-09\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 6\n",
      "Number of objective gradient evaluations             = 6\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 1\n",
      "Total seconds in IPOPT                               = 1.217\n",
      "\n",
      "EXIT: Optimal Solution Found.\n"
     ]
    }
   ],
   "source": [
    "optimize!(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364aa14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd39ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b859a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a889b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
